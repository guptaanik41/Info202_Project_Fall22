{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "bce6a584-6aaa-4a0d-97b2-3c7a3e296304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as np\n",
    "import seaborn as sns\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "e341a3d6-3ccf-451b-9d0a-b2bbc6c32051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20030219</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20030219</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20030219</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text\n",
       "0      20030219  aba decides against community broadcasting lic...\n",
       "1      20030219     act fire witnesses must be aware of defamation\n",
       "2      20030219     a g calls for infrastructure protection summit\n",
       "3      20030219           air nz staff in aust strike for pay rise\n",
       "4      20030219      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"abcnews-date-text.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "ca02074c-6a84-4ab5-98c5-4178cd0ed8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the publish_date column is of no use to us\n",
    "df = df[['headline_text']];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4453e0b-0d5c-43ee-bd09-f6671ee4a382",
   "metadata": {},
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "8434b613-d98a-4b2f-b41f-531019d3c89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using stemming and lemmatization techniques\n",
    "def LemStem(doc):\n",
    "    \n",
    "    # the stemmer requires a language parameter\n",
    "    s = SnowballStemmer('english')\n",
    "    \n",
    "    lemmatize_words = WordNetLemmatizer().lemmatize(doc, pos='v')\n",
    "    return s.stem(lemmatize_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "92c56ab2-b00a-440e-99d1-14a12ead6c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreProcess(doc):\n",
    "    \n",
    "    processed_words = gensim.utils.simple_preprocess(doc,min_len=2, max_len=20)  \n",
    "    \n",
    "    stopWords = gensim.parsing.preprocessing.STOPWORDS\n",
    "    pre_processed = [LemStem(token) for token in processed_words if token not in stopWords]\n",
    "    \n",
    "    return pre_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "d4a38fc8-6bdc-4f69-adcf-2bcdaeeb3fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the processed text\n",
    "processed_text = df['headline_text'].apply(PreProcess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010b3631-a3d3-460e-b3ce-1d3edeae1c2c",
   "metadata": {},
   "source": [
    "### Creating Word Vectors with Bag of Words Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "fd31a083-b07f-4e9f-9c0b-7ab93559b5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dictionary containes the mapping of all the words, i.e. token with their respective integer ids.\n",
    "dic = gensim.corpora.Dictionary(processed_text)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "2c4f3ff1-c9fa-4aac-8466-773b7d4f8283",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic.filter_extremes(no_below=15, no_above=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "36337fe1-e677-415e-8481-b8aa51df1167",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = [dic.doc2bow(doc) for doc in processed_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134cfe84-e8ef-4b98-85e6-14dcbae6a495",
   "metadata": {},
   "source": [
    "### Topic Modeling with LDA using BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "561eef0f-e563-4777-bf79-175e42c3ec47",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_topic_modeling_bow = gensim.models.LdaMulticore(bag_of_words,\n",
    "                    num_topics=10, id2word=dic, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "062352e2-e0fd-4c0f-9c56-e5d85b1477a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing index and topics\n",
    "def print_topics(model):\n",
    "    for idx, topic in model.print_topics():\n",
    "        print(idx)\n",
    "        print(topic)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "92bf4899-a489-4e8b-a3ac-6fd7528a3ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.055*\"council\" + 0.029*\"execut\" + 0.029*\"secur\" + 0.029*\"heritag\" + 0.029*\"protect\" + 0.029*\"fail\" + 0.029*\"posit\" + 0.029*\"chief\" + 0.029*\"tas\" + 0.029*\"dargo\"\n",
      "\n",
      "1\n",
      "0.025*\"rise\" + 0.025*\"air\" + 0.025*\"australia\" + 0.025*\"aust\" + 0.025*\"iraq\" + 0.025*\"nz\" + 0.025*\"strike\" + 0.025*\"aid\" + 0.025*\"unit\" + 0.025*\"celebr\"\n",
      "\n",
      "2\n",
      "0.036*\"climb\" + 0.036*\"subway\" + 0.036*\"death\" + 0.036*\"korean\" + 0.036*\"toll\" + 0.036*\"toughen\" + 0.036*\"continu\" + 0.036*\"south\" + 0.036*\"organ\" + 0.036*\"conduct\"\n",
      "\n",
      "3\n",
      "0.031*\"carew\" + 0.031*\"ruin\" + 0.031*\"goal\" + 0.031*\"opp\" + 0.031*\"freak\" + 0.031*\"timet\" + 0.031*\"lock\" + 0.031*\"fate\" + 0.031*\"roma\" + 0.031*\"war\"\n",
      "\n",
      "4\n",
      "0.046*\"welcom\" + 0.046*\"council\" + 0.024*\"commonwealth\" + 0.024*\"calleri\" + 0.024*\"final\" + 0.024*\"dent\" + 0.024*\"cut\" + 0.024*\"decis\" + 0.024*\"fix\" + 0.024*\"tie\"\n",
      "\n",
      "5\n",
      "0.037*\"kuwait\" + 0.037*\"suppli\" + 0.037*\"big\" + 0.037*\"troop\" + 0.037*\"paroo\" + 0.037*\"combat\" + 0.037*\"plan\" + 0.037*\"daili\" + 0.037*\"british\" + 0.037*\"water\"\n",
      "\n",
      "6\n",
      "0.019*\"council\" + 0.019*\"affect\" + 0.019*\"stosur\" + 0.019*\"ambiti\" + 0.019*\"nz\" + 0.019*\"homeless\" + 0.019*\"wast\" + 0.019*\"memphi\" + 0.019*\"travel\" + 0.019*\"strike\"\n",
      "\n",
      "7\n",
      "0.037*\"alp\" + 0.037*\"tell\" + 0.037*\"win\" + 0.037*\"laker\" + 0.037*\"bryant\" + 0.037*\"shut\" + 0.037*\"doubl\" + 0.037*\"critic\" + 0.037*\"crean\" + 0.037*\"lead\"\n",
      "\n",
      "8\n",
      "0.032*\"protect\" + 0.032*\"communiti\" + 0.032*\"iraqi\" + 0.032*\"summit\" + 0.032*\"decid\" + 0.032*\"dem\" + 0.032*\"plebiscit\" + 0.032*\"hold\" + 0.032*\"infrastructur\" + 0.032*\"broadcast\"\n",
      "\n",
      "9\n",
      "0.031*\"hop\" + 0.031*\"launceston\" + 0.031*\"big\" + 0.031*\"harass\" + 0.031*\"championship\" + 0.031*\"brigadi\" + 0.031*\"troop\" + 0.031*\"dismiss\" + 0.031*\"report\" + 0.031*\"wit\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_topics(lda_topic_modeling_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed76f17-22f7-411b-916e-4ad2f7dc551d",
   "metadata": {},
   "source": [
    "### Creating Word Vectors with TF-IDF Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "223b2506-17d8-4348-997c-4ef19a7e1618",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = models.TfidfModel(bow_corpus)\n",
    "\n",
    "# Applying the above tranformation to the entire corpus\n",
    "tf_idf_corpus = tf_idf[bag_of_words] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8357216-9e4c-411d-83d0-53cf20af421e",
   "metadata": {},
   "source": [
    "### Topic Modeling with LDA using TF-IDF technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "bcbc2120-7ca6-4140-bfae-9cbdce0bee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_topic_modeling_tfidf = gensim.models.LdaMulticore(tf_idf_corpus, num_topics=10, id2word=dictionary,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "88b0874f-aedf-44a2-9c97-c18e6095baa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.011*\"season\" + 0.010*\"bomber\" + 0.007*\"access\" + 0.007*\"intern\" + 0.007*\"brown\" + 0.007*\"find\" + 0.006*\"industri\" + 0.006*\"ahead\" + 0.006*\"honour\" + 0.006*\"test\"\n",
      "\n",
      "1\n",
      "0.008*\"presid\" + 0.008*\"stay\" + 0.008*\"educ\" + 0.007*\"port\" + 0.007*\"tri\" + 0.007*\"lawyer\" + 0.007*\"plan\" + 0.007*\"kangaroo\" + 0.006*\"hop\" + 0.006*\"food\"\n",
      "\n",
      "2\n",
      "0.008*\"bomber\" + 0.007*\"beach\" + 0.007*\"price\" + 0.007*\"push\" + 0.007*\"adelaid\" + 0.007*\"club\" + 0.006*\"deal\" + 0.006*\"court\" + 0.006*\"strong\" + 0.001*\"access\"\n",
      "\n",
      "3\n",
      "0.008*\"prison\" + 0.008*\"august\" + 0.008*\"releas\" + 0.008*\"worker\" + 0.007*\"prepar\" + 0.007*\"million\" + 0.007*\"eas\" + 0.007*\"bomber\" + 0.007*\"melbourn\" + 0.006*\"alleg\"\n",
      "\n",
      "4\n",
      "0.008*\"drop\" + 0.008*\"access\" + 0.008*\"blaze\" + 0.007*\"shop\" + 0.007*\"get\" + 0.007*\"sector\" + 0.007*\"land\" + 0.007*\"green\" + 0.007*\"news\" + 0.006*\"final\"\n",
      "\n",
      "5\n",
      "0.007*\"bomber\" + 0.007*\"investig\" + 0.007*\"live\" + 0.007*\"leak\" + 0.006*\"return\" + 0.006*\"busi\" + 0.006*\"confirm\" + 0.006*\"centr\" + 0.006*\"boss\" + 0.006*\"strong\"\n",
      "\n",
      "6\n",
      "0.008*\"studi\" + 0.007*\"sector\" + 0.007*\"economi\" + 0.006*\"bendigo\" + 0.006*\"child\" + 0.006*\"record\" + 0.006*\"retail\" + 0.006*\"futur\" + 0.006*\"polit\" + 0.006*\"jam\"\n",
      "\n",
      "7\n",
      "0.007*\"miner\" + 0.007*\"record\" + 0.006*\"charg\" + 0.006*\"bail\" + 0.006*\"miss\" + 0.006*\"open\" + 0.006*\"action\" + 0.006*\"lawyer\" + 0.006*\"sydney\" + 0.006*\"fish\"\n",
      "\n",
      "8\n",
      "0.008*\"kill\" + 0.008*\"servic\" + 0.008*\"western\" + 0.007*\"storm\" + 0.006*\"asylum\" + 0.006*\"seek\" + 0.006*\"latest\" + 0.006*\"role\" + 0.006*\"track\" + 0.006*\"boat\"\n",
      "\n",
      "9\n",
      "0.007*\"region\" + 0.007*\"arrest\" + 0.007*\"charg\" + 0.006*\"line\" + 0.006*\"accus\" + 0.006*\"govt\" + 0.001*\"bomber\" + 0.001*\"blaze\" + 0.001*\"access\" + 0.001*\"adelaid\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_topics(lda_topic_modeling_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f80c70-fdd2-48bc-a013-660da751ae09",
   "metadata": {},
   "source": [
    "### Assigning topics to each heading using the topic modeling technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "8cfb9f8d-254a-4cdf-ac2c-8b4ff53120f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample data is selected and grounded coding is done on this dataset\n",
    "df=df[0:40]\n",
    "\n",
    "def max_prob(topic_prob):\n",
    " \n",
    "    topic_prob.sort(key = lambda x: x[1])\n",
    "    return topic_prob[-1][0]\n",
    " \n",
    "    \n",
    "topics = []\n",
    "for i in range(len(bag_of_words)):\n",
    "    topic_prob = lda_topic_modeling_bow.get_document_topics(bag_of_words[i], \n",
    "                minimum_probability=None, minimum_phi_value=None, per_word_topics=False)\n",
    "    \n",
    "    # finding the topic with maximum probability\n",
    "    topic = max_prob(topic_prob)\n",
    "    \n",
    "    topics.append(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "2a0202bf-0419-4cf0-86b0-75f3b8ee150d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text  Topic\n",
       "0  aba decides against community broadcasting lic...      8\n",
       "1     act fire witnesses must be aware of defamation      9\n",
       "2     a g calls for infrastructure protection summit      8\n",
       "3           air nz staff in aust strike for pay rise      1\n",
       "4      air nz strike to affect australian travellers      6"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding the topic as a column\n",
    "df['Topic']=topics\n",
    "df.drop(['publish_date'],axis=1,inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "5c273a28-4fc8-44a9-8929-34b91455dfe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    8\n",
       "1    5\n",
       "4    5\n",
       "8    4\n",
       "9    4\n",
       "3    4\n",
       "0    4\n",
       "5    2\n",
       "7    2\n",
       "2    2\n",
       "Name: Topic, dtype: int64"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "2044091b-2ae1-439d-813d-3adb1f2739a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Topic_modeling.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
